% !TeX root=../main.tex
\chapter{روش تحقیق}

\section{تعریف مسئله}
\label{section_problem}
این بخش توصیف رسمی مسئله‌ی نگاشت شبکه مجازی را ارائه می‌دهد که در این گزارش در نظر گرفته شده است.

\subsection{شبکه‌ٔ فیزیکی}
یک گراف بدون جهت 
$G^{p}=(\mc{N}^{p}, \mc{L}^{p})$
برای توصیف کلی یک شبکه فیزیکی با تعداد $S$ گره‌ٔ فیزیکی (یعنی سرور‌ها)  $\mc{N}^{p}=\{n_{1}^{p},\dots,n_{S}^{p} \}$  و تعداد $T$ پیوند فیزیکی $\mc{L}^{p}=\{ \ell_{1}^{p}, \dots, \ell_{T}^{p} \}$ در نظر گرفته می‌شود.
هر گره برای خود یک مجموعه $\mc{R}$ منابع (یعنی مقدار \lr{CPU} و \lr{RAM}) دارد که مقدار منابع $r\in\mc{R}$  در گره‌های $n_{i}^{p}$ برابر است با $f_{r}(n_{i}^{p})$.  پهنای باند پیوند $\ell_{i}^{p}$ برابراست با $b(\ell_{i}^{p})$. 
علاوه بر این، ما $\mc{P}_{i,j}$ را مجموعه ای از همه مسیرها بین گره‌های فیزیکی $n_{i}^{p}$ و $n_{j}^{p}$ تعریف می‌کنیم تا  جایی که هر مسیر $p_{i,j}\in\mc{P}_{i,j}$ لیستی از پیوندهای فیزیکی است.همچنین،  $\mc{Q}_{i}$ بیانگر مجموعه تمام مسیرهایی است که حاوی پیوند فیزیکی $\ell_{i}^{p}$ است. ما از اعداد واقعی برای نمایش منابع فیزیکی و ظرفیت‌ پهنای باند لینک‌ها با توجه به یک واحد پایه استفاده می‌کنیم.

\subsection{شبکهٔ مجازی}
فرض می‌کنیم شبکه مجازی 
$G^{t}=(\mc{N}^{t}, \mc{L}^{t})$
در زمان 
$t\in\mc{T}$
می‌رسد. در حالی که 
$\mc{T}$ 
نشان‌دهنده‌ٔ مجموعه‌ای از زمان‌های رسیدن شبکه‌های مجازی مختلف می‌باشد. در اینجا $\mc{N}^{t}=\{ n_{1}^{t},\dots,n_{U}^{t} \}$ و  $\mc{L}^{t}=\{ \ell_{1}^{t}, \dots, \ell_{V}^{t} \}$ به‌ترتیب مجموعه‌ٔ گره‌های مجازی $U$ و مجموعه‌ٔ پیوند‌های مجازی $V$ را نشان می‌دهند. 
$\varepsilon(\ell_{i}^{t})=\{a,b\}$
مجموعه‌ی دو گره‌ مجازی $n_{a}^{t}$ و  $n_{b}^{t}$ را نشان می‌دهد که دو سر یک لینک مجازی $\ell_{i}^{t}$ هستند.
هر گره مجازی $n_{i}^{t}$ برای عملکرد خود به $g_{r}(n_{i}^{t})$ واحد منبع $r$ برای انجام عملیات‌هایش و هر پیوند مجازی $\ell_{i}^{t}$ به مقدار $d(\ell_{i}^{t})$ واحد پهنای باند را برای مدیریت ارتباطات خود مصرف می‌کند. بعلاوه، فرض می‌کنیم شبکه مجازی $G^{t}$ پس از $\tau_{t}$ واحد کار خود را تمام کرده و منابع فیزیکی اکتسابی خود را با خروج از شبکه آزاد کند. ما از اعداد واقعی برای نمایش نیازهای منبع مجازی و پهنای باند با توجه به واحد پایه استفاده می‌کنیم.
\subsection{مسئله‌ٔ نگاشت}
هر شبکه مجازی، در بدو ورود در زمان $ t $، باید صریحاً رد یا پذیرفته شود. برای پذیرش یک شبکه مجازی، گره‌ها و پیوندهای مجازی باید به ترتیب به گره‌های فیزیکی و مسیرهایی با ظرفیت منابع کافی ترسیم شوند. اگر $ a_t $ متغیر تصمیم باینری را نشان دهد که نشان دهنده وضعیت پذیرش $ G ^ {t} $ باشد، یک الگوریتم نگشات شبکه مجازی باید بدون دانستن ورود شبکه‌های مجازی پس از زمان $ t $، مقدار $ a_t $ را تعیین کند. فرض کنید $x_{i,t}^{j}$ یک متغیر تصمیم باینری باشد که زمانی که گره مجازی $n_{i}^{t}$ درگره فیزیکی $n_{j}^{p}$ تعبیه می‌شود مقدارش برابر $1$ شود. به همین ترتیب، متغیر تصمیم دودویی $y_{i,t}^{j,k}$ تعریف شده است تا نشان دهد آیا پیوند مجازی $\ell_{i}^{t}$ در مسیر فیزیکی $p_{j,k}$ بین گره‌های فیزیکی $ n_ {j} ^ {p} $ و $ n_ {k} ^ {p} $ تعبیه شده‌است یا نه. یک نگاشت شبکه مجازی معتبر محدودیت‌های زیر را برآورده می‌کند،
\begin{gather}
f_{r}(n_{j}^{p}) 
\ge 
\sum_{\substack{t'\in\mc{T}\\t'\le t}}
\sum_{n_{i}^{t'} \in \mc{N}^{t'}}
a_{t'}
\delta_{t',\tau_{t'}}^{t}
x_{i,t'}^{j}
g_{r}(n_{i}^{t'}),
\quad
\forall
n_{j}^{p} \in \mc{N}^{p},
r\in \mc{R} \label{eq_1} \\
y_{i,t}^{j,k} 
\le 
\sum_{\substack{a,b\in\varepsilon(\ell_{i}^{t})\\a\ne b}}
x_{a,t}^{j}
x_{b,t}^{k},
\qquad
\forall 
\ell_{i}^{t} \in \mc{L}^{t},
n_{j}^{p},n_{k}^{p}\in\mc{N}^{p},
p_{j,k} \in \mc{P}_{i,j} \label{eq_2} \\
b(\ell_{i}^{p})
\ge
\sum_{\substack{t'\in\mc{T}\\t'\le t}}
\sum_{\ell_{j}^{t'} \in \mc{L}^{t'}}
\sum_{\substack{p_{k,m} \\ \ell_{i}^{p}\in \mc{Q}_{i}}}
a_{t'}
\delta_{t',\tau_{t'}}^{t}
y_{j,t'}^{k,m}
d(\ell_{j}^{t'}),
\qquad
\forall 
\ell_{i}^{p} \in \mc{L}^{p} \label{eq_3}
\end{gather}
که در اینجا $\delta_{t',\tau_{t'}}^{t}$ یک تابع کمکی است که مشخص می‌کند آیا شبکه مجازی $G^{t'}$ با مدت زمان $\tau_{t'}$ در زمان $t$ فعال است یا نه.
محدودیت 
\eqref {eq_1}
 اطمینان می‌دهد که ظرفیت هر نوع منبع در همه گره‌های فیزیکی رعایت می‌شود. محدودیت 
\eqref {eq_2} 
 تضمین می‌کند که هر پیوند مجازی به یک مسیر فیزیکی ترسیم می‌شود که گره‌های فیزیکی اتصال دهنده، دو نقطه انتهایی پیوند مجازی را به هم متصل می‌کند. محدودیت 
 \eqref {eq_3}
  ظرفیت پهنای باند پیوندهای فیزیکی را اعمال می‌کند. اگر هر یک از این محدودیت‌ها نقض شود، $ G ^ {t} $ مسدود می‌شود.
  
  
  در این کار، به حداقل رساندن احتمال انسداد شبکه مجازی هدف مسئله تلقی می‌شود، که با یک استراتژی تخصیص منبع کارآمد بدست آمده است. این تخصیص منابع در طی مراحل نگاشت حداقل مقدار منابع فیزیکی را هدر می‌دهد. به طور کلی، نسبت پذیرش به شکل زیر تعریف می‌شود.
  

\begin{equation}
\lim_{\vert\mc{T}\vert\rightarrow \infty} \frac{\sum_{t\in\mc{T}} a_t}{\vert\mc{T}\vert}.
\end{equation}

%
بعلاوه، \gls{Revenue} و  \gls{Cost} الگوریتم را می‌توان به صورت زیر محاسبه کرد:

\begin{gather}
\text{Revenue} 
=
\sum_{t\in\mc{T}}
a_{t} \Big\{
\sum_{n_{i}^{t} \in \mc{N}^{t}}
\sum_{r\in\mc{R}}
\zeta_{r}
g_{r}(n_{i}^{t})
+
\sum_{\ell_{i}^{t} \in \mc{L}^{t}}
d(\ell_{i}^{t})\Big\}, \label{eq_rev} \\
\begin{split}
\text{Cost} =
\sum_{t\in\mc{T}} &
a_{t} \Big\{
\sum_{n_{i}^{t} \in \mc{N}^{t}}
\sum_{r\in\mc{R}}
\xi_{r}
g_{r}(n_{i}^{t}) + 
\sum_{n_{i}^{p}, n_{j}^{p} \in \mc{N}^{p}}
\sum_{p_{i,j} \in \mc{P}_{i,j}}
\sum_{\ell_{k}^{t} \in \mc{L}^{t}}
y_{k,t}^{i,j}
d(\ell_{i}^{t})\vert p_{i,j}\vert\Big\},
\end{split}
\label{eq_cost} 
\end{gather}

که در آن $\vert p_{i,j}\vert$ طول مسیر $p_{i,j}$ و $\zeta_r \ge 0$ و $\xi_r \ge 0$ می‌توانند برای تنظیم هزینه و درآمد منبع نوع $r$ با توجه به هزینه‌  در آمد پهنای مورد استفاده قرار بگیرند.



\section{پیش‌زمینه}

در این بخش، به طور خلاصه پیش زمینه‌ای را که در بقیه مقاله مورد نیاز است شرح می‌دهیم. برای جزئیات بیشتر و بحث لطفا به
  \cite {graphSage} و \cite {argva}
  مراجعه کنید.

\subsection{شبکه‌های عصبی گرافی فضایی}
فرض کنید مجموعه‌ای $ K $ از توابع جمع کننده $\Psi_{k}(.)$ در دسترس است که می‌تواند با کمک ماتریس‌های وزنی $W^k$، اطلاعات عمق $ K $ام هر گره را جمع‌آوری کند . شیوه محاسبه  $\Psi_{k}(.)$  و $W^k$  بعداً توضیح داده می‌شود.
بگذارید $ x_v $ لیست ویژگی‌های گره $ v $ را نشان دهد. همچنین، بگذارید  $ h_v ^ k $ نشان دهنده اطلاعات ترکیبی درباره گره $ v $ و همسایگی عمق $ k $ ام آن باشد. در ابتدا، $ h_v ^ 0 = x_v $ و تجمع اطلاعات به صورت افزایشی و به صورت جمعی برای همه گره‌های شبکه اتفاق می‌افتد، که هر گره از اطلاعات جمع شده عمق $ k $ از همسایگان خود برای دستیابی به عمق  $(k+1)$ خود استفاده می‌کند (مشابه عملیات پشتیبان‌گیری که در هسته اصلی رویکردهای یادگیری تقویت قرار دارند 
\cite {sutton2018reinforcement}
). به طور خاص، هر گره $ v $ معادلات زیر را $ K $ بار ارزیابی می‌کند.
\begin{gather}
\psi \gets \Psi_{k}(\{ h_u^k : u\text{ of neighbor a is }v \}), \label{eq_agg} \\
h_v^{k+1} \gets \alpha\left( W^{k+1} . h_v^k\frown\psi \right), \label{eq_sigma}
\end{gather}
در این معادله $\psi$ یک مکان یاب میانی است، $\alpha(.)$ یک \gls{af} غیرخطی است و $\frown$ عملگر ادغام لیست است. توجه داشته باشید که معادله ~\eqref{eq_sigma} مشابه یک لایه شبکه عصبی کاملاً متصل است، جایی که $ W ^ {k 1} $ ماتریس وزن آن است. برای کنترل زمان مصرف و میزان مصرف حافظه این روش، معمولاً در هر مرحله تجمیع، فقط یک نمونه تصادفی با اندازه ثابت از همسایگان در نظر گرفته می‌شود به جای محاسبه $ psi $ برای همه همسایگان از آن استفاده می‌شود. 
گزینه‌های متعددی برای توابع تجمع وجود دارد. جمع کننده‌های \lr{LSTM} و \lr{Pooling} گزینه‌های رایجی هستند. به عنوان مثال، جمع کننده میانگین، میانگین عنصر بردارهای $ h_u ^ {k} $ در معادله  \eqref {eq_agg} است.
سپس، \gls{stochastic gradient descent algorithm} به یک \gls{loss function}ی مبتنی بر گراف اعمال می‌شود تا ماتریس‌های وزنی $ W ^ k $ و پارامترهای $\Psi_{k}(.)$ را بیاموزد. فرض کنید  $ z_u = h_u ^ {K} $، تابع ضرر برای گره $ u $ به این صورت تعریف شده است،
\begin{gather}
-\log{(s(z_u^{\intercal}z_v))} - Q\mathbb{E}_{w}[\log{(s(-z_u^{\intercal}z_w))}],
\end{gather}
که در آن $ u $ از یک \gls{random walk} با طول ثابت از $ v $ به دست می‌آید، $ Q $ تعداد نمونه‌های همسایگان $ u $، $ w $ یک همسایه نمونه و $ s (.) $  تابع \gls{lsgd} است.

\subsection{\gls{vgae}}
در این روش، یک گره به‌جای یک نمایش واحد به یک توزیع نگاشت می‌شود. سپس، یک نمایش تصادفی از توزیع گرفته شده برای بازسازی گره اصلی استفاده می‌شود. در نهایت، با به حداقل رساندن خطای بازسازی، یک توزیع نمایش بهینه از نمودار به دست می‌آید.
فرض کنید $\pmb{Z}$ ماتریس بردارهای نمایش $ z_u $ باشد. یک روش معمول برای طراحی رمزگذار، مدل‌سازی توزیع $\pmb{Z}$ است که به مقادیر ویژگی‌های گره $\pmb{X}$ و ساختار گراف $\pmb{A}$ (که با $q(\pmb{Z}\vert \pmb{A},\pmb{X})$ نشان داده می‌شود.) وتوسط یک توزیع گاوسی چند متغیره مشروط می‌شود. سپس، می‌توان از دو شبکه عصبی نمودار برای یادگیری پارامترهای توزیع استفاده کرد، یعنی میانگین $\pmb{\mu}$ و واریانس لگاریتم $\log{\pmb{\sigma}^2}$. 
یک رمزگشای ساده با $p(\pmb{A}\vert \pmb{Z})$ نشان داده می‌شود توسط فرمول
$s(\pmb{ZZ}^{\intercal})$
تعریف می‌شود. 
سپس خطای باز‌سازی توسط فرمول زیر محاسبه می‌شود.
\begin{gather}
\mathbb{E}_{q(\pmb{Z}\vert \pmb{X},\pmb{A})}\big[\log{p(\pmb{A}\vert \pmb{Z})}\big]
-
\text{KL}\big(q(\pmb{Z}\vert \pmb{X},\pmb{A}) \parallel p(\pmb{Z})\big),
\label{train_reconstruct}
\end{gather}
که در آن $KL(.)$ واگرایی \lr{Kullback-Leibler} است و $p(\pmb{Z})$ توزیع قبلی است،یعنی احتمال مشاهده  $\pmb{Z}$ در صورتی که $\pmb{\mu}=0$ ($ I $ ماتریس هویت است).

\subsection{\gls{adverserial}}
می‌توان عملکرد اتوانکدر را با استفاده از توزیع $q(\pmb{Z}\vert \pmb{A},\pmb{X})$ به عنوان یک مدل تولیدکننده در یک بازی متخاصمانه به طرز قابل توجهی بهبود بخشید. برای این منظور، یک مدل \gls{discriminator} (مثلاً یک شبکه عصبی چند لایه) برای تمایز نمونه‌های تولید‌کننده از نمونه‌های توزیع قبلی $q(\pmb{Z}\vert \pmb{A},\pmb{X})$ استفاده می‌شود. هدف از این بازی به عنوان تعریف شده است، 
\begin{equation}
\min\limits_{\mc{G}}
\max\limits_{\mc{D}} 
\,\mathbb{E}_{\pmb{Z}\sim p(\pmb{Z})}\big[\log{\mc{D}(\pmb{Z})}\big]
-
\mathbb{E}_{\pmb{X}\sim p(\pmb{X})}\big[\log{\mc{D}(1-\mc{G}(\pmb{X},\pmb{A}))}\big]
\label{eq_adversarial}
\end{equation}
که در آن $\mc{G}$ و $\mc{D}$ به ترتیب مدل‌های تولید کننده و جداکننده هستند. برای اطمینان از مطابقت مقادیر نهفته با توزیع قبلی، می‌توان از معادله 
\eqref {eq_adversarial} 
در روند آموزش $q(\pmb{Z}\vert \pmb{A},\pmb{X})$ استفاده کرد.


\section{الگوریتم \lr{VNE}}
در این بخش، ما یک الگوریتم مقیاس پذیر بر اساس اتوانکودر خودکار گرافی برای حل مسئله نگاشت شبکه مجازی  (به بخش \ref {section_problem} مراجعه کنید) ارائه می‌دهیم.
ما برای محاسبه  نمایش هر سرور فیزیکی که شامل اطلاعاتی در مورد: (۱) ظرفیت منابع سرور، (۲) ظرفیت منابع سرورهای همسایه تا فاصله مورد نظر،و (۳)  توپولوژی شبکه متصل شده و ظرفیت پهنای باند آن می‌باشد از یک معماری \gls{argva} استفاده می‌کنیم. سپس،گره‌های فیزیکی را بر اساس نمایش‌های بدست آمده \gls{خوشه‌بندی} می‌کنیم، که سرورهای یک خوشه دارای یک منبع و ظرفیت پهنای باند مشابه هستند. در نتیجه، سرورهای موجود در یک خوشه شرایط مشابهی را برای نگاشت یک شبکه مجازی فراهم می‌کنند. سپس، از هر خوشه یک سرور تصادفی انتخاب می‌کنیم که الگوریتم نگاشت شبکه مجازی فرآیند خود را بر روی آن آغاز می‌کند و در نهایت بهترین نقطه شروع با کمترین هزینه به عنوان راه‌حل انتخاب می‌شود. در انتها نیز نمایش سرور و خوشه‌بندی به طور کارآمد به‌روز می‌شود.


برای افزایش مقیاس پذیری، مجموعه ای از معیارهای خوشه بندی را تعریف می‌کنیم که می‌تواند مصالحه  زمان اجرا - کیفیت را کنترل کند. یک معیار، تازگی خوشه است که به خوشه اجازه می‌دهد اساساً از خوشه قبلی مجدداً استفاده کند و خوشه را فقط پس از یک تغییر مهم به‌روز کند. به عنوان مثال، می‌توان به زمانی که ظرفیت یک سرور یا یک لینک به طور قابل توجهی تغییر می‌کند اشاره کرد.

در ادامه، مدل \gls{encoder}، مدل جدا کننده، روش خوشه بندی و الگوریتم نگاشت را توصیف می‌کنیم.

\subsection{رمزکننده}
توپولوژی مدل پیشنهادی ما برای محاسبه نمایش سرورهای فیزیکی (یعنی مدل رمزکننده) در شکل 
\ref {fig_encoder} 
نشان داده شده است.

\begin{figure}[t]
	\centering
	\scalebox{0.22}{
		\tikzfig{encoder}
	}
	\caption{مدل رمزکننده با لایه‌های شبکه عصبی گرافی فضایی}
	\label{fig_encoder}
\end{figure}

رمزکننده ماتریس ظرفیت سرور منابع $ X $ (یعنی \lr{CPU}، \lr{GPU} و \lr{RAM}) و ماتریس همجواری وزنی $ A $ از شبکه فیزیکی را دریافت می‌کند، که در آن وزن یال‌ها پهنای باند را به عنوان ورودی نشان می‌دهند و توزیعی‌ای ایجاد می‌کنند که نمایش‌های سرور را تعیین می‌کند.
مشابه معماری‌های موجود، ما از توزیع چند متغیره گاوسی برای مدل‌سازی توزیع نمایش‌ها استفاده می‌کنیم.
در نظر داشته باشید که محاسبه توزیع نمایش، به جای تولید مستقیم بازنمایی، به طور قابل توجهی تعمیم پذیری مدل را بهبود می‌بخشد. بنابراین، وقتی ظرفیت یک پیوند موجود تغییر می‌کند، مدل باز هم نمایش مناسبی از سرور‌ها را ایجاد می‌کند.
به طور خاص، رمز‌کننده یک شبکه عصبی گرافی فضایی با دو لایه است. که در آن، لایه دوم برای محاسبه میانگین و واریانس توزیع نمایش به دو قسمت تقسیم می‌شود.
اندازه کانال ورودی در لایه اول با تعداد منابع سرور برابر است (یعنی $\mc{R}$). همچنین، اندازه کانال‌های ورودی و خروجی در هر دو قسمت لایه دوم برابر با 16 است.
تابع جمع کننده  $\Psi_k(.)$  به شکل زیر تعریف می‌شود.
\begin{equation}
W_1 \pmb{x}^{k-1}(n) + W_2 . \frac{1}{|\mc{L}(n)|}
\sum_{\ell\in\mc{L}(n)} b(\ell) \pmb{x}^{k-1}(m)
\end{equation}

که در آن $ W_1 $ و $ W_2 $ ماتریس‌های پارامترهای قابل یادگیری هستند، $\pmb{x}^{k-1}(n)$ بردار ویژگی سرور $ n $ پس از $ k-1 $ دور جمع‌آوری اطلاعات از گره‌های اطراف است . $\mc{L}(n)$ مجموعه تمام پیوندهایی است که به سرور $ n $ متصل می‌شوند و $ m $ دومین نقطه پایانی پیوند فیزیکی $\ell$ است.
توجه داشته باشید که مقدار اولیه بردار ویژگی،  یعنی $\pmb{x}^{0}(n)$، به عنوان لیستی از ظرفیت منابع سرور $ n $، یعنی $\pmb{f}(n)$  تنظیم شده است. 
مشاهده کنید که از پهنای باند پیوند $b(\ell)$ برای کنترل تأثیر در دسترس بودن منابع به سرورهای مجاور استفاده می‌شود. تابع فعال سازی لایه‌های اول و دوم به ترتیب \lr{ReLU} و خطی هستند.


در پایان، تعداد مراحل تجمع $ K $ برابر با قطر مورد انتظار شبکه‌های مجازی انتخاب شده است. در شبکه‌های تصادفی در نظر گرفته شده در بخش 
\ref {section_results}
، این مقدار کمی بیشتر از $ 2 $ بود و از این رو رمزکننده در شکل 
 \ref {fig_encoder} 
 دارای دو لایه است. توجه داشته باشید که تعداد لایه‌ها در رمزکننده قطر \lr{VN}‌ها را محدود نمی‌کند و بنابراین این روش در مورد \lr{VN}‌ها با قطر دلخواه اعمال می‌شود. تعداد لایه‌ها فقط بر عمق اطلاعات همسایگان تأثیر می‌گذارد.
 
\subsection{\gls{decoder} و جداکننده}

شمای کلی اتوانکدر به کار گرفته شده در شکل \ref{fig_discriminator} ارائه شده است.
\begin{figure}[t]
	\centering
	\scalebox{0.22}{
		\tikzfig{clustering}
	}
	\caption{مدل اتوانکدر با شبکه‌های عصبی کاملا متصل شده به عنوان رمزگشا و جداکننده }
	\label{fig_discriminator}
\end{figure}

بعد از اینکه رمزکننده توزیع نمایش سرور‌ها را تعیین کرد، نمایشی نمونه $ Z '$ ترسیم می‌شود و به رمزگشای داده می‌شود. این رمزگشا یک شبکه عصبی با دو لایه نورون است. توجه داشته باشید که در اینجا، ما نمی‌توانیم از GNN استفاده کنیم، زیرا ورودی $ Z $ یک  
\emph {نمایش} 
 از گراف است است و نه یک گراف. لایه دوم رمزگشا دارای دو قسمت است،که یکی از آنها ماتریس مجاورت را بازسازی می‌کند و لایه دیگر ماتریس منابع سرور یعنی $X$ را بازسازی می‌کند. سپس خروجی‌ها با ورودی‌های اصلی مقایسه می‌شوند تا خطای بازسازی را محاسبه کرده و رمزکننده را با معادله
  \eqref {train_reconstruct}
   آموزش دهد. لایه اول رمزگشا 16  نورون دارد و هر قسمت 8 نورون دارد.
   همچنین، $ Z '$ به یک مدل جداکننده به عنوان ورودی می‌شود، که در لایه پایین شکل
\ref {fig_discriminator}
   نشان داده شده است. این مدل یک شبکه عصبی رو به جلو متراکم ۳ لایه با 16، 32 و 16 نورون در هر لایه است. تابع فعال‌ساز در لایه‌های تفکیک کننده \lr{ReLU} است.
   
  جداکننده آموزش دیده است تا نمونه‌های نماینده توزیع قبلی $ p (Z) $ را از نمونه‌های نمایشی تولید شده توسط رمزکننده تشخیص دهد. به طور همزمان، رمزکننده آموزش داده می‌شود (برای دیدن نمونه‌هایی که توزیع قبلی را دنبال می‌کنند، به معادله
   \eqref {eq_adversarial}
    مراجعه کنید). این بهینه‌سازی مشترک رمزکننده و جداکننده از طریق چارچوب متخاصمانه، اطمینان حاصل می‌کند که رمزکننده از توزیع انتخاب شده قبلی پیروی کند.
    
    \subsection{خوشه‌بندی}
    
    نمونه نمایش $ Z '$ تولید‌شده توسط رمزکننده برای خوشه‌بندی سرورهای فیزیکی در مجموعه ای از گروه‌ها استفاده می‌شود. می‌توان گفت سرورهای داخل یک گروه مشابه یکدیگر و متفاوت از سرورهای گروه‌های دیگر هستند.
    %
    به طور خاص، سرورهای درون یک گروه ظرفیت و  پهنای باند مشابهی را برای انجام عملیات نگاشت  تا یک فاصله خاص فراهم می‌کنند. بنابراین‌، وقتی دو سرور فیزیکی مختلف را در همان گروه نقطه شروع روند نگاشت انتخاب کنیم، هزینه عملیات نگاشت تغییر چندانی نمی‌کند.
    %
    ما برای انجام خوشه‌بندی از الگوریتم \lr{k-means} استفاده می‌کنیم . به طوری که تعداد خوشه‌ها با استفاده از روش \lr{elbow} 
    \cite {elbowMethod} 
    تعیین می‌شود. این روش مجموع مربع فاصله نمونه‌ها از نزدیکترین مرکز خوشه را برای تعداد مختلف مراکز خوشه ارزیابی می‌کند. این روش در نهایت عددی را انتخاب می‌کند که بیشترین بهبود را نسبت به مقدار قبلی داشته باشد. در شبکه‌هایی که برای ارزیابی در نظر گرفتیم، تعداد خوشه‌ها به طور عمومی بین ۴ تا ۶ بود. 
 
\subsection{نگاشت}
    روش نگاشت شبکه مجازی پیشنهادی، به نام \ourAlg، در الگوریتم  \ref{alg} شرح داده شده است.
\lr{
	\begin{algorithm}[t]
		\caption{\ourAlg\ -- \ourAlgFull}
		\label{alg}
		\DontPrintSemicolon\SetNoFillComment
		\SetKwFunction{procedureName}{\textbf{\ourAlgName}} 
		\SetKwProg{myalg}{procedure}{}{}    
		\myalg{\procedureName{%
				$G^{p}$, $G^{t}$, $Z$, $\mc{C}$, $\mc{M}$%
		}}{
			cost$^{\star}$ $\gets \infty$ \\
			sol$^{\star}$ $\gets$ null \\
			\For{$C \in \mc{C}$}{ \label{alg_line_for_start}
				$n \gets$ start\_node($C$) \label{alg_line_n} \\
				cost, sol $\gets$ embed($n, G^{p}, G^{t}$) 
				\hfill \text{\fontsize{9}{9}\selectfont /* See Alg.~\ref{alg_embed} */} \\
				\If{cost $<$ cost$^{\star}$}
				{
					cost$^{\star} \gets$ cost \\
					sol$^{\star} \gets$ sol \\
				}
			} \label{alg_line_for_end}
			allocate($G^{p}$, $G^{t}$, sol$^{\star}$) \label{alg_line_allocate} \\
			\If{\text{Available resource of a server changed by more than $\kappa_1\%$} \textbf{or} \text{available bandwidth of a link changed by more than $\kappa_2\%$}}{
				$Z \gets$ re\_GNN($\mc{M}$, $G^p$, sol$^{\star}$) \label{alg_line_reGNN} \\
				$\mc{C} \gets$ re\_cluster($Z$, sol$^{\star}$) \label{alg_line_reCluster}
			}
			\textbf{Return} sol$^{\star}$, $Z$, $\mc{C}$
		}{}
	\end{algorithm}
}


وقتی شبکه مجازی $ G ^ {t} $ در زمان $ t $ می‌رسد، به همراه شبکه فیزیکی $ G ^ {p} $، نمایش سرورها $ Z $، خوشه‌بندی سرورها $\mc{C}$ و مدل اتوانکدر $\mc{M}$ به الگوریتم  \ourAlg  به عنوان ورودی داده می‌شود.
%
حلقه  \lr{for} در خطوط 
\ref {alg_line_for_start}
 تا 
\ref {alg_line_for_end}،
نگاشت با کمترین هزینه را پیدا و ذخیره می‌کند. به طور خاص  \ourAlg\ از هر خوشه در خط \ref{alg_line_n}  یک سرور فیزیکی انتخاب می‌کند و آن را به برنامه دیگری به نام \lr{ \textbf {embed}} منتقل می‌کند که در الگوریتم \ref{alg_embed} شرح داده شده است.
%%
برنامه نگاشت از سرور ورودی فیزیکی به عنوان نقطه شروع روش جاسازی خود بر اساس مکانیزم \gls{bfs} استفاده می‌کند.
ما دو آستانه  $\alpha\times|\mc{N}^{t}|$ و $\beta$ را تعریف کردیم تا به ترتیب تعداد کل سرورهای بازرسی شده و حداکثر عمق جستجو را محدود کنیم.

پس از یافتن بهترین راه حل،  \ourAlg\  آن را در شبکه فیزیکی موجود در خط \ref {alg_line_allocate} اعمال می‌کند. سپس \ourAlg\ تغییر منابع و پهنای باند موجود را بررسی می‌کند. وقتی هر یک از این موارد به ترتیب بیش از $\kappa_1\%$ یا $\kappa_2\%$ تغییر کند، ماتریس نمایش $ Z $ و خوشه‌بندی $\mc{C}$ به روز می‌شوند تا در دور بعدی عملیات نگشات در خطوط  \ref{alg_line_reGNN} و \ref{alg_line_reCluster} استفاده شوند.
این معیارها به   \ourAlg\ اجازه می‌دهد تا خوشه بندی را هنگامی که یک تغییر اساسی در شبکه ایجاد می‌کند، مجدداً محاسبه کند و از محاسبات غیرضروری که ممکن است مقیاس پذیری را محدود کند، جلوگیری کند.
%
توجه داشته باشید که، در شبکه‌های عصبی گرافی فضایی، نمایش‌ها به صورت محلی برای هر گره محاسبه می‌شوند، که کاملاً قابل موازی‌سازی است. برای مثال، می‌توان از کتابخانه‌هایی مانند 
\lr{PyTorch Scatter} ~ \cite {pytorch_scatter}
 برای تسریع در بروزرسانی مدل اتوانکدر $\mc{M}$ و ماتریس نمایش $ Z $ استفاده کرد.
 \lr{
\begin{algorithm}[t]
	\caption{Embedding Subroutine}
	\label{alg_embed}
	\DontPrintSemicolon\SetNoFillComment
	\SetKwFunction{procedureName}{\textbf{embed}} 
	\SetKwProg{myalg}{procedure}{}{}    
	 \myalg{\procedureName{%
			$n$, $G^{p}(\mc{N}^{p}, \mc{L}^{p})$, $G^{t}(\mc{N}^{t}, \mc{L}^{t}), \alpha, \beta$%
	}}{
		$q_{p} \gets$ queue($n$) \\
		$q_{t} \gets$ prio\_queue($\mc{N}^{t}$) 
		\hfill \text{/* Prioritizes virtual nodes with higher CPU demand */} \\
		\While{True}{
			$n_{p}$ $\gets q_{p}$.pop() \\
			Mark $n_{p}$ visited \\
			\While{True}{
				$n_{t} \gets q_{t}$.pop() \\
				% \For{$m' \in$ neighbor($n_{t}$)}{
				%     \If{$m'$ is not embedded}{
				%         $q_{t}$.push($m'$)
				%     }            
				% }
				\If{$\pmb{g}(n_{t}) > \pmb{f}(n_{p})$}{
					Break \hfill \text{/* Not enough resource */}
				}
				\If{$\nexists$ path to connect $n_{t}$ to its neighbors}{
					Break \hfill \text{/* Not enough bandwidth */}
				}
				Embed $n_{t}$ in $n_{p}$ and store the decision in sol
			}
			\If{dist($n_{p}$, $n$) = $\beta$}{
				\textbf{continue}
			}
			counter $\gets 0$ \\
			\For{$n' \in$ neighbor($n_{p}$) \textbf{and} \text{counter} $\leq (\alpha \times |\mc{N}^{t}|)^{\frac{1}{\beta}}$}{
				\If{$n'$ is not visited}{
					$q_{p}$.push($n'$) \\
					counter $\gets$ counter $+ 1$
				}
			}
		}
		cost $\gets$ Calculate the cost with Eq.~\eqref{eq_cost} \\
		\textbf{Return} cost, sol
	}{}
\end{algorithm}
}




    